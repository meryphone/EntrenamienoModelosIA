## Gradient Boosting Machine (GBM)

```{r}
set.seed(1234)
suppressWarnings({
gbm_model <- train(
    V16 ~ .,                        # Reemplaza `V16` con tu variable objetivo si es diferente
    data = credit.Datos.Train.Normalizados,
    method = "gbm",
    trControl = control,
    tuneLength = 5,
    verbose = FALSE                 # Evita imprimir detalles durante el entrenamiento
)})

```

```{r}
print(gbm_model)
# Realizar predicciones en el conjunto de prueba
gbm_predictions <- predict(gbm_model, newdata = credit.Datos.Test.Normalizados)

# Calcular la matriz de confusión
confusionMatrix(gbm_predictions, credit.Datos.Test.Normalizados$V16)

```

El modelo tiene una precisión del 84.67%, lo que significa que predice
correctamente el 84.67% de los casos. La sensibilidad es del 78.95%,
indicando que identifica bien los negativos, mientras que la
especificidad es del 91.8%, reflejando una buena capacidad para detectar
positivos. El valor predictivo positivo es del 92.31%, lo que muestra
que la mayoría de las predicciones negativas son correctas, y el valor
predictivo negativo es del 77.78%, indicando que algunos positivos
predichos podrían no ser precisos. La métrica Kappa, de 0.6951, sugiere
un buen nivel de concordancia entre predicciones y valores reales.
Aunque los resultados son sólidos, la sensibilidad podría mejorarse para
equilibrar mejor las predicciones.

### Preprocesado para mejorar GBM

Para intentar mejorar los resultados obtenidos por GBM vamos a realizar
un nuevo preprocesado de manera experimental a ver que logramos obtener.

Para comenzar, vamos a eliminar las variables que el modelo considera
menos importantes

```{r}
# Calcular la importancia de las variables directamente desde gbm
importancia_directa <- summary(gbm_model$finalModel)  # Si entrenaste con caret

```

Como podemos ver, las variables V13, V6 y V4 son de una baja importancia
para el modelo, además, nos advertía de esto previamente con diferentes
$warning$ al entrenarlo, por lo que hemos tenido que poner
$suppressWarning$ por motivos de pulcridad de la salida.

```{r}
credit.Datos.Train.gbm = credit.Datos.Train
credit.Datos.Test.gbm = credit.Datos.Test

# Eliminar las columnas específicas por nombre
credit.Datos.Train.gbm <- credit.Datos.Train.gbm[, !colnames(credit.Datos.Train.gbm) %in% c("V4", "V6", "V13")]
credit.Datos.Test.gbm <- credit.Datos.Test.gbm[, !colnames(credit.Datos.Test.gbm) %in% c("V4", "V6", "V13")]

```

Tras eliminar las variables, procedemos a hacer otra prueba de
entrenamiento para ver los resultados del modelo:

```{r}
set.seed(1234)
suppressWarnings({
gbm_model <- train(
    V16 ~ .,                        # Reemplaza `V16` con tu variable objetivo si es diferente
    data = credit.Datos.Train.gbm,
    method = "gbm",
    trControl = control,
    tuneLength = 5,
    verbose = FALSE                 # Evita imprimir detalles durante el entrenamiento
)})

print(gbm_model)
# Realizar predicciones en el conjunto de prueba
gbm_predictions <- predict(gbm_model, newdata = credit.Datos.Test.gbm)

# Calcular la matriz de confusión
confusionMatrix(gbm_predictions, credit.Datos.Test.gbm$V16)

```

Como podemos ver, hemos mejorado tanto la precisión como la kappa. Para
intentar mejorarlo aún más vamos a proceder a aplicar otro tipo distinto
de normalización distinta al que ya habíamos probado anteriormente.
Vamos a realizar una normalización usando $preProcess$ de caret con el
metodo range:

```{r}
# Aplicar la transformación Box-Cox a las variables numéricas
preprocess <- preProcess(credit.Datos.Train.gbm, method = "range")

# Transformar los datos de entrenamiento
credit.Datos.Train.gbm.normalizado <- predict(preprocess, credit.Datos.Train.gbm)
credit.Datos.Test.gbm.normalizado <- predict(preprocess, credit.Datos.Test.gbm)


# Verificar el resultado
summary(credit.Datos.Test.gbm.normalizado)

```

La normalización ha escalado correctamente las variables numéricas entre
0 y 1. Procedemos a volver a evaluar el modelo con esta nueva
transformación:

```{r}
set.seed(1234)

gbm_model <- train(
    V16 ~ .,                        # Reemplaza `V16` con tu variable objetivo si es diferente
    data = credit.Datos.Train.gbm.normalizado,
    method = "gbm",
    trControl = control,
    tuneLength = 5,
    verbose = FALSE                 # Evita imprimir detalles durante el entrenamiento
    )

# Realizar predicciones en el conjunto de prueba
gbm_predictions <- predict(gbm_model, newdata = credit.Datos.Test.gbm.normalizado)

# Calcular la matriz de confusión
confusionMatrix(gbm_predictions, credit.Datos.Test.gbm.normalizado$V16)

```

Como vemos, esta transformación no ha mejorado el modelo, sino que se ha
mantenido todo igual tanto kappa con la precisión de clasificación. La
normalización no ha afectado el rendimiento del modelo GBM porque este
algoritmo no depende de la escala de las variables, ya que trabaja con
árboles de decisión que dividen los datos en función de umbrales. Aunque
no mejora directamente el modelo, puede ayudar a estabilizar cálculos
numéricos, facilitar la interpretación de las variables y preparar los
datos para combinar con otros algoritmos que sí sean sensibles a la
escala. En este caso, su impacto ha sido neutral porque GBM es robusto a
diferencias en la escala de las variables.

Podríamos probar a reducir el sesgo de las variables, sin embargo, la
reducción del sesgo no mejora el modelo GBM porque este algoritmo es
robusto a distribuciones sesgadas, ya que toma decisiones basadas en
divisiones por umbrales y no en la escala o forma de las variables.
Además, las transformaciones aplicadas podrían reducir la variabilidad
importante en las variables, afectando su capacidad predictiva. En este
caso, el rendimiento del modelo probablemente está limitado por otros
factores.

Por último, para intentar mejorar aún más el rendimiento con este
preprocesado vamos a intentar "tunear" los hiperparámetros a ver que
conseguimos obtener. 

Para comenzar vamos a analizar los hiperparámetros que podemos modificar:
```{r}
modelLookup(("gbm"))
```

Como observamos tenemos cuatro hiperpárametros disponibles para configurar. Los cuatro hiperparámetros que podemos ajustar en el modelo gbm son: **n.trees**, que define el número de iteraciones o árboles en el proceso de boosting, lo que controla la complejidad y capacidad de ajuste del modelo; **interaction.depth**, que establece la profundidad máxima de cada árbol, determinando hasta qué punto el modelo puede capturar interacciones complejas entre variables; **shrinkage**, también conocido como tasa de aprendizaje, que reduce el impacto de cada árbol en la predicción final, ayudando a evitar el sobreajuste y permitiendo que el modelo aprenda de manera más gradual; y **n.minobsinnode**, que especifica el número mínimo de observaciones requeridas en un nodo terminal, lo cual ayuda a regular la complejidad de los árboles y reduce el riesgo de sobreajuste ajustando la granularidad de las divisiones. Estos parámetros nos permiten controlar la flexibilidad y precisión del modelo.

Vamos a aplicar una **grid search** enfocada en buscar las combinaciones de hiperparámetros cercanas a la elegida automáticamente por el modelo para intentar conseguir mejor rendimiento.

```{r}
# Grid ajustado alrededor de los valores óptimos
tune_grid <- expand.grid(
  n.trees = c(90, 100, 110),            # Ajuste en el número de árboles
  interaction.depth = c(2, 3, 4),       # Ajuste leve en profundidad
  shrinkage = c(0.08, 0.1, 0.12),       # Tasa de aprendizaje cercana a 0.1
  n.minobsinnode = c(8, 10, 12)         # Observaciones mínimas por nodo
)

# Validación cruzada más robusta
control <- trainControl(method = "cv", number = 10)  # Validación cruzada de 10 pliegues

# Entrenar el modelo GBM
set.seed(1234)
gbm_tuned <- train(
  V16 ~ ., 
  data = credit.Datos.Train.gbm.normalizado,
  method = "gbm",
  trControl = control,
  tuneGrid = tune_grid,
  verbose = FALSE
)

plot(gbm_tuned)

```

El número de iteraciones (\# Boosting Iterations), tasa de aprendizaje
(shrinkage), y número mínimo de observaciones por nodo (n.minobsinnode)
afectan la precisión del modelo en validación cruzada. Observamos que el
rendimiento es más consistente y alto para combinaciones con
interaction.depth = 4, shrinkage = 0.1, y n.minobsinnode = 8,
especialmente alrededor de 100 iteraciones. Esto sugiere que estas
configuraciones balancean bien la capacidad predictiva y la
generalización del modelo, mientras que ajustes más bajos en
interaction.depth o n.minobsinnode tienden a reducir la precisión.

Vamos a ver la precisión sobre el conjunto de test:

```{r}
# Realizar predicciones en el conjunto de prueba
gbm_predictions <- predict(gbm_tuned, newdata = credit.Datos.Test.gbm.normalizado)

# Calcular la matriz de confusión
confusionMatrix(gbm_predictions, credit.Datos.Test.gbm.normalizado$V16)


```

Los resultados muestran una **precisión** del **87.59%** en el conjunto
de test, lo que indica un buen rendimiento del modelo. La sensibilidad
es del 85.53%, lo que refleja que el modelo identifica correctamente la
mayoría de los casos positivos, y la especificidad es del 90.16%, lo que
muestra una buena capacidad para detectar negativos. El valor predictivo
positivo (PPV) del 91.55% y el valor predictivo negativo (NPV) del
83.33% confirman que las predicciones son confiables en ambas clases. El
índice Kappa de 0.7508 sugiere un acuerdo sustancial entre predicciones
y datos reales, y el p-valor del test de McNemar (0.332) indica que no
hay diferencias significativas entre las tasas de error de las clases.
Estos resultados validan que el modelo tiene un buen equilibrio entre
sensibilidad y especificidad, logrando una precisión balanceada del
87.85%.
